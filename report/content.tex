\section{算法基本思路}
统计二字, 三字在训练材料中的出现次数, 依此对句子进行估值. 使用模拟退火法搜索出估值函数值最大的句子.

\subsection{估值函数}
对输入数据进行处理得出$m^{fw}_{i,j}$和$m^{bw}_{i,j}$, 表示子串$\overline{ij}$出现的次数. 

其中$fw$表示正向, $bw$表示将整个句子反过来. 

$j$为单字, $i$的长度为$1$时为双字模型, $i$的长度为$2$时为三字模型.

对于每个正/反向的词语$\overline{ij}$, 其估值为

$$ Value_{\overline{ij}} = \frac{m_{i,j}}{S_{i,p(j)}} * \log_{B}{B+S} $$

其中

$$ S_{i,p(j)} = \sum_{\forall k, p(k) = p(j)}m_{i,k} $$

$p(k)$表示汉字$k$对应的拼音.

$B$ 为某一确定常数.

这个公式由两部分组成. 左边部分表示$j$在上一两个字为$i$时出现的条件概率. 右边为一个取值范围较小的系数, 表示串$i$后接一个拼音为$p(j)$的汉字的权重, 也从某种程度上反映$i$的词频.

对于一个答案串$|T|=n$, 同时使用正反向, 双三字模型时, 可以得到$4n$个权重. 取最大的$k_s*4n$个相加, 作为这个串的权值. 其中$k_s$为某固定常数. 由于一个句子可能分为多个部分, 部分间的连接权值可能会误导最终的答案, 故使用了这种方法来避免这样的影响. 

\subsection{退火}
由于估值函数较为复杂, 不便使用动态规划, 所以使用了模拟退火的方法来求得一个较优的解. 

模拟退火的寻找相邻答案的过程如下.

每次先从左到右再从右到左遍历当前答案, 并根据上个字$i$, 以$exp(k_c * \frac{m_{i,j}}{S_{i,p(j)}})$的概率选择新的字符, 计算新的$Value_{\overline{ij}} = V_n$. 其中$k_c$为另一固定常数. 若旧的连接估值$V_o < V_n$ 或 $ random() * \frac{V_o}{V_n} < learning\_rate$, 则用新字符替换当前的字符.

其中$learning\_rate$使用等比下降的方式. 在优化过程中发现前一部分下降较慢, 后一部分下降较快时, 效果较好.

\section{实验效果}
在提供的学生生成的数据集中, 三字模型的字准确率达$78.5\%$, 句准确率达$36.6\%$, 二字模型的字准确率达$69.5\%$, 句准确率达$16.5\%$.

下面分析一些典型的错误.

\subsection{上下文关联性差}
\paragraph{输入} \texttt{dian liang shu zi ren sheng}

\paragraph{期望} 点亮数字人生

\paragraph{输出} 电量数字人生

\paragraph{分析} 第一个词和后面的词关联性较低. 而 "电量" 的词频又比 "点亮" 更高, 所以程序选择了 "电量".

\paragraph{解决方法} 需要引入语义理解的方法.

\subsection{多音字}
\paragraph{输入} \texttt{ma dai ma}

\paragraph{期望} 码代码

\paragraph{输出} 三字模型: 马大妈, 双字模型: 么大妈

\paragraph{分析} "大" 为多单字. 而 "大妈" 词频非常高. 

\paragraph{解决方法} 引入有正确注音的训练数据, 对词语对应的拼音进行分类.

\subsection{完全不一样}
\paragraph{输入} \texttt{gou li guo jia sheng si yi}

\paragraph{期望} 苟利国家生死以

\paragraph{输出} 够立国家生死一 

\paragraph{分析} 这句古诗在训练数据里没出现过.

\paragraph{解决方法} 更全或者更精准的训练数据.

\section{参数选择和性能分析}
算法中涉及到一些参数. 我主要使用了手工调整的方法来优化参数. 

\subsection{精确度}
采用句准确率, 字准确率和ROUGE-L作为指标来衡量准确率.

在相同参数下, 三者的性能如下表.

\begin{tabular}{|c|ccc|}
	\hline
	指标(\%) & 三字 & 精简三字 & 双字 \tabularnewline \hline
	句准确率 & 1 & 2 & 3 \tabularnewline 
	字准确率 & 1 & 2 & 3 \tabularnewline
	ROUGE-L  & 1 & 2 & 3 \tabularnewline \hline
\end{tabular}

上文提到的$k_s$对结果也有一定影响. 选取不同的 $k_s$时得到的指标如下表.

\begin{tabular}{|c|ccccc|}
	\hline
	指标(\%) & $k_s=0.6$ & $k_s=0.7$ & $k_s=0.8$ & $k_s=0.9$ & $k_s=1.0$ \tabularnewline \hline
	句准确率 & 1 & 2 & 3 & 3 & 3 \tabularnewline
	字准确率 & 1 & 2 & 3 & 3 & 3 \tabularnewline
	ROUGE-L  & 1 & 2 & 3 & 3 & 3 \tabularnewline \hline
\end{tabular}

得出结论$k_s=0.8$时效果最好.

\subsection{运行性能分析}
三字模型的映射数据大小高达约\texttt{800MB}, 使用\texttt{pickle}格式存储, 加载需要一分钟左右. 为了优化时间, 我将数据中出现频率较低的词直接删除, 得到了一个大小约为\texttt{400MB}的精简三字映射. 而双字模型的映射大小不超过\texttt{100MB}.

完整三字模型加载后占用内存约\texttt{7GB}.

三字模型模拟退火算法运行速度较慢. 模拟退火参数下降约$50$次, 每次迭代$20n$次随机修改. 一条六个字的输入需要约$1s$的时间.

\section{感想和优化空间}
这是我第一次写模拟退火算法. 也是难得用python写这么多代码. 因为对调参并没有充足的理解, 所以花费了一天的时间来调模拟退火. 然后调整估价函数也花费了一天的时间. 我想随着经验的积累我可以在这些方面做得更好更快.

我的程序在性能上也比较差. 由于采用了不太熟悉的python和pickle所以没有想到比较系统的优化. 但是我想这是人工智能课, 重点还是算法本身, 所以也没有太过关注优化.

我想优化空间在于以下几处.

\begin{itemize}
	\item 更加合理的估价函数
	\item 引入分词和多音字辨识的训练数据
	\item 使用神经网络来减少暴力存概率表的空间开销
	\item 优化python的map的性能
\end{itemize}
